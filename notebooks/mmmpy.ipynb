{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "# Get hourly files from June 1 and June 2 2021\n",
    "now = datetime.datetime(2021, 6, 1, 0)\n",
    "utc_end = datetime.datetime(2021, 6, 2, 0)\n",
    "\n",
    "while now < utc_end:\n",
    "    cmd = (\n",
    "        'wget -L -O ' + now.strftime('%Y%m%d%H') + '.zip ' +\n",
    "        'https://mrms.agron.iastate.edu/' + now.strftime('%Y/%m/%d/%Y%m%d%H') + '.zip'\n",
    "    )\n",
    "    print(cmd)\n",
    "    subprocess.run(cmd, shell=True)\n",
    "    now += datetime.timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import subprocess\n",
    "\n",
    "url = \"https://mrms.agron.iastate.edu/2022/08/23/2022082300.zip\"\n",
    "TMP_ROOT = Path(\"/tmp/mmmpy/\")\n",
    "TMP_DIR = \"/tmp/mmmpy-tmp/\"\n",
    "\n",
    "class IAState:\n",
    "    def __init__(self, targetdir=TMP_ROOT) -> None:\n",
    "        self.targetdir = targetdir\n",
    "        self.tmpdir = Path(TMP_DIR)\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        if not self.tmpdir.exists():\n",
    "            self.tmpdir.mkdir(parents=True)\n",
    "\n",
    "        if not self.targetdir.exists():\n",
    "            self.targetdir.mkdir(parents=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if self.tmpdir.exists():\n",
    "            ...\n",
    "            # shutil.rmtree(self.tmpdir)\n",
    "\n",
    "    def get(self, url: str, raise_for_status=True):\n",
    "        _, filename = url.rsplit(\"/\", maxsplit=1)\n",
    "        outfile = self.tmpdir / filename#.removesuffix(\".zip\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with outfile.open(\"wb\") as f:\n",
    "                # shutil.copyfileobj(r.raw, fdst)\n",
    "                for chunk in r.iter_content(chunk_size=1024): \n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        f.write(chunk)\n",
    "        # return self.__unzip(outfile)\n",
    "\n",
    "    # def download_url(self, url: str, chunk_size=128):\n",
    "    #     r = requests.get(url, stream=True)\n",
    "    #     _, filename = url.rsplit(\"/\", maxsplit=1)\n",
    "    #     with (self.targetdir / filename).open(\"wb\") as fd:\n",
    "    #         for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "    #             fd.write(chunk)\n",
    "\n",
    "    def __unzip(self, filename: str):\n",
    "        \"\"\"\n",
    "        -FF\n",
    "        --fixfix\n",
    "            Fix the zip archive. The -F option can be used if some\n",
    "            portions of the archive are missing, but requires a reasonably\n",
    "            intact central directory.   The  input  archive is scanned as\n",
    "            usual, but zip will ignore some problems.  The resulting\n",
    "            archive should be valid, but any inconsistent entries will be\n",
    "            left out.\n",
    "\n",
    "            When doubled as in -FF, the archive is scanned from the\n",
    "            beginning and zip scans  for  special  signatures  to\n",
    "            identify  the  limits between the archive members. The single\n",
    "            -F is more reliable if the archive is not too much damaged, so\n",
    "            try this option first.\n",
    "\n",
    "            If  the archive is too damaged or the end has been truncated,\n",
    "            you must use -FF.  This is a change from zip 2.32, where the\n",
    "            -F option is able to read a truncated archive.  The -F option\n",
    "            now more reliably fixes archives with minor damage and the -FF\n",
    "            option is  needed to fix archives where -F might have been\n",
    "            sufficient before.\n",
    "            ...\n",
    "        \"\"\"\n",
    "\n",
    "        with subprocess.Popen(\n",
    "            [\n",
    "                \"zip\",\n",
    "                \"-FF\",\n",
    "                TMP_DIR + filename,\n",
    "                # (self.tmpdir / filename).as_posix(),\n",
    "                \"--out\",\n",
    "                filename\n",
    "            ],\n",
    "        ):\n",
    "            ...\n",
    "\n",
    "\n",
    "# cmd = f\"zip -FFv /tmp/mmmpy-de1beace-234e-11ed-b5a2-0242ac110002/2022082300.zip --out /tmp/mmmpy/2022082300.zip\"\n",
    "# subprocess.run(cmd, shell=True)\n",
    "with IAState() as s:\n",
    "    s.get(url)\n",
    "# filename= \"2022082300.zip\"\n",
    "# with subprocess.Popen(\n",
    "#     [\n",
    "#         \"zip\",\n",
    "#         \"-q\",\n",
    "#         \"-FF\",\n",
    "#         TMP_DIR+filename,\n",
    "#         # \"/tmp/mmmpy-tmp/2022082300.zip\",\n",
    "#         \"--out\",\n",
    "#         filename,\n",
    "#     ]\n",
    "# ):  ...\n",
    "\n",
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "Bad magic number for file header",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8408/306544080.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/mmmpy-tmp/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/mmmpy/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0mfheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructFileHeader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_FH_SIGNATURE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mstringFileHeader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1533\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bad magic number for file header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzef_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_FH_FILENAME_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: Bad magic number for file header"
     ]
    }
   ],
   "source": [
    "tmp, = Path(\"/tmp/mmmpy-tmp/\").glob(\"*\")\n",
    "with zipfile.ZipFile(tmp) as z:\n",
    "    z.extractall(\"/tmp/mmmpy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "# def unzip (path, total_count):\n",
    "#     for root, dirs, files in os.walk(path):\n",
    "#         for file in files:\n",
    "#             file_name = os.path.join(root, file)\n",
    "#             if (not file_name.endswith('.zip')):\n",
    "#                 total_count += 1\n",
    "#             else:\n",
    "#                 currentdir = file_name[:-4]\n",
    "#                 if not os.path.exists(currentdir):\n",
    "#                     os.makedirs(currentdir)\n",
    "#                 with ZipFile(file_name) as zipObj:\n",
    "#                     zipObj.extractall(currentdir)\n",
    "#                 os.remove(file_name)\n",
    "#                 total_count = unzip(currentdir, total_count)\n",
    "#     return total_count\n",
    "import gzip\n",
    "f = Path(\"/tmp/mmmpy/2022082300.zip\")\n",
    "\n",
    "import zipfile\n",
    "z = zipfile.ZipFile(\"/tmp/mmmpy/2022082300.zip\")\n",
    "# z.infolist()\n",
    "# z.extractall()\n",
    "f = [f.filename for f in z.infolist() if not f.filename.endswith(\".gz\")][0]\n",
    "z.extract(member=\"2022082300/ANC/ANC_FinalForecast/\")\n",
    "\n",
    "# z.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f= open(\"/tmp/mmmpy/2022082300.zip\")\n",
    "zipfile.is_zipfile(\"/tmp/mmmpy/2022082300.zip\")\n",
    "# gzip.\n",
    "with ZipFile(\"/tmp/mmmpy/2022082300.zip\") as myzip:\n",
    "    with myzip.open('eggs.txt') as myfile:\n",
    "       eggs = io.TextIOWrapper(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.content\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "with io.BytesIO(r.content) as buffer:\n",
    "    with zipfile.ZipFile(buffer) as z:\n",
    "        z.extractall(\"/tmp/mmmpy\")\n",
    "# from pathlib import Path\n",
    "\n",
    "# # import zipfile\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# tmpdir = Path(\"/tmp/mmmpy\")\n",
    "# with (tmpdir / \"2022082300.zip\").open(\"wb\") as fdst:\n",
    "#     shutil.copyfileobj(r.raw, fdst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple(tmpdir.glob(\"*\"))file\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(tmpdir / \"2022082300.zip\", ) as z:\n",
    "    print(z)\n",
    "\n",
    "# file = tmpdir / \"2022082300.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "FILE_NAME_PATTERN = re.compile(r\"/([A-Za-z]+(?:-|_)?[A-Za-z]+)+\")\n",
    "data = Path(os.path.abspath(__name__)).parents[1] / \"data\"\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted((data / \"MRMS_MergedReflectivity\").glob(\"*grib2\"))[:4]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dims(ds: xr.Dataset) -> xr.Dataset:\n",
    "    duplicates = [\"heightAboveSea\"]\n",
    "    # if more than one file was passed the valid_time should be greater than 1\n",
    "    if ds.valid_time.size > 1:\n",
    "        # for which we add a new validTime dimension\n",
    "        ds = ds.expand_dims({\"validTime\": ds[\"valid_time\"].to_numpy()})\n",
    "        duplicates.append(\"validTime\")\n",
    "\n",
    "    return ds.drop(\"valid_time\").drop_duplicates(duplicates)\n",
    "\n",
    "\n",
    "def name(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if len(ds.data_vars) != 1:\n",
    "        # mrms grib2 data should only have one variable\n",
    "        raise Exception\n",
    "    (ds_name,) = ds\n",
    "    # not storing history, will use the history object to infer a name\n",
    "    hist = ds.attrs.pop(\"history\", None)\n",
    "    # if a name was not explicility provided\n",
    "    # if not name:\n",
    "    # use the known name if unknow infer one from the file name\n",
    "    if ds_name != \"unknown\":\n",
    "        name = ds_name\n",
    "    else:\n",
    "        name_list = FILE_NAME_PATTERN.findall(hist)\n",
    "        if name_list:\n",
    "            name = name_list[-1]\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    return ds.rename({ds_name: name})\n",
    "\n",
    "\n",
    "def main():\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            files,\n",
    "            chunks={},\n",
    "            engine=\"cfgrib\",\n",
    "            data_vars=\"minimal\",\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"heightAboveSea\"],\n",
    "            backend_kwargs=dict(\n",
    "                mask_and_scale=True,\n",
    "                decode_times=True,\n",
    "                concat_characters=True,\n",
    "                decode_coords=True,\n",
    "                # use_cftime=\"%Y-%m\",\n",
    "                decode_timedelta=None,\n",
    "                lock=None,\n",
    "                indexpath=\"{path}.{short_hash}.idx\",\n",
    "                filter_by_keys={},\n",
    "                read_keys=[],\n",
    "                encode_cf=(\"parameter\", \"time\", \"geography\", \"vertical\"),\n",
    "                squeeze=True,\n",
    "                time_dims={\"valid_time\"},\n",
    "            ),\n",
    "        )\n",
    "        .pipe(dims)\n",
    "        .pipe(name)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ds = main()\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfds = ds.copy()\n",
    "mfds.to_zarr(data / \"mfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(data / \"mfds\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    for file in files[:4]:\n",
    "        ds1 = xr.open_dataset(\n",
    "            file,\n",
    "            engine=\"cfgrib\",\n",
    "            chunks={},\n",
    "            # data_vars=\"minimal\",\n",
    "            # combine=\"nested\",\n",
    "            # concat_dim=[\"heightAboveSea\"],\n",
    "            backend_kwargs=dict(\n",
    "                mask_and_scale=True,\n",
    "                decode_times=True,\n",
    "                concat_characters=True,\n",
    "                decode_coords=True,\n",
    "                # use_cftime=\"%Y-%m\",\n",
    "                decode_timedelta=None,\n",
    "                lock=None,\n",
    "                indexpath=\"{path}.{short_hash}.idx\",\n",
    "                filter_by_keys={},\n",
    "                read_keys=[],\n",
    "                encode_cf=(\"parameter\", \"time\", \"geography\", \"vertical\"),\n",
    "                squeeze=True,\n",
    "                time_dims={\"valid_time\"},\n",
    "            ),\n",
    "        )\n",
    "        yield ds1.expand_dims(\n",
    "            {\n",
    "                \"validTime\": [ds1[\"valid_time\"].to_numpy()],\n",
    "                \"heightAboveSea\": [ds1[\"heightAboveSea\"].to_numpy()],\n",
    "            }\n",
    "        ).drop_duplicates([\"validTime\", \"heightAboveSea\"]).drop(\"valid_time\").pipe(\n",
    "            name\n",
    "        )\n",
    "\n",
    "\n",
    "datasets = tuple(generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xr.concat(datasets,dim=\"validTime\").drop_duplicates(\"validTime\").groupby(\"validTime\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zarr\n",
    "\n",
    "teststore = data / \"test-bucket\"\n",
    "group_name = \"3DRefl\"\n",
    "\n",
    "if teststore.exists():\n",
    "    shutil.rmtree(teststore)\n",
    "import numpy as np\n",
    "for vt, ds in xr.concat(datasets,dim=\"validTime\").drop_duplicates(\"validTime\").groupby(\"validTime\"):\n",
    "    (dsname,) = ds\n",
    "    ds = ds.expand_dims({\"validTime\":[vt]}).fillna(np.nan)\n",
    "\n",
    "    ds[\"MRMS_MergedReflectivityQC\"].attrs.clear() \n",
    "    if not teststore.exists():\n",
    "        ds.to_zarr(\n",
    "            teststore,\n",
    "            mode=\"a\",\n",
    "            group=group_name,\n",
    "            compute=True,\n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        ds.drop([\"latitude\",\"longitude\",\"heightAboveSea\"]).to_zarr(\n",
    "            teststore,\n",
    "            mode=\"a\",\n",
    "            group=group_name,\n",
    "            append_dim=\"validTime\",\n",
    "            compute=True,\n",
    "        )\n",
    "\n",
    "xr.open_zarr(teststore / group_name, consolidated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "paths = pd.Series((data / \"MRMS_MergedReflectivity\").glob(\"*.grib2\"))\n",
    "vt=pd.to_datetime(paths.apply(lambda p: p.name).str.extract(r\"(\\d{8}-\\d{6})\",expand=False))\n",
    "for vt,x in paths.groupby(vt):\n",
    "    print(tuple(x))\n",
    "    # print(x)\n",
    "# pd.to_datetime(for p in paths)#pd.Series(x.name for x in (data / \"MRMS_MergedReflectivity\").glob(\"*.grib2\")).str.extract(r\"(\\d{8}-\\d{6})\",expand=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(Group)\n",
    "import zarr\n",
    "root:zarr.Group = zarr.group(teststore / group)\n",
    "def unpack_root()->tuple[zarr.Array,...]:\n",
    "    yield from (root[key] for key in (\"latitude\",\"longitude\",\"heightAboveSea\",\"validTime\"))\n",
    "x,y,z,t = unpack_root()\n",
    "tuple(z)\n",
    "# x,y,z,t :tuple[zarr.Array,...]= root[\"latitude\"], root[\"longitude\"],  root[\"heightAboveSea\"], root[\"validTime\"],\n",
    "# x,y\n",
    "# z.values\n",
    "# g:zarr.Group = zarr.open(teststore / group)\n",
    "# tuple(g.array_keys())\n",
    "# g.info.obj\n",
    "# g.info_items()\n",
    "# g.store.items()\n",
    "# g.c\n",
    "# vt: np.datetime64 = ds[\"validTime\"].values[0]\n",
    "# (vt,) = pd.to_datetime(ds[\"validTime\"].values).strftime(\"%Y-%m-%dT\")\n",
    "\n",
    "# ds[\"validTime\"].to_numpy().astype(\"datetime64[m]\")\n",
    "# vt\n",
    "# np.str\n",
    "# import zarr\n",
    "\n",
    "# g: zarr.Group = zarr.open(teststore / \"3DRefl\")\n",
    "# cs: zarr.DirectoryStore = g.chunk_store\n",
    "# g.get(\"validTime\").size\n",
    "# # print(g.get(\"validTime\"))\n",
    "# # tuple(g)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0717c53d6b06231a47d82f28bf5c85ba985f82808dfef8e7f3be1cf4215fce4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0717c53d6b06231a47d82f28bf5c85ba985f82808dfef8e7f3be1cf4215fce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

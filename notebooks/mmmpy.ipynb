{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache \n",
    "from mmmpy import MRMSFeatures, MRMSRegions\n",
    "import numpy as np\n",
    "import enum\n",
    "import pandas as pd\n",
    "f'{MRMSFeatures[\"H60_Above_M20C\"]}'\n",
    "pd.Series({\"H60_Above_-20C\":1})[MRMSFeatures[\"H60_Above_M20C\"].value]\n",
    "list([MRMSFeatures.BrightBandBottomHeight.ANC_ConvectiveLikelihood])\n",
    "\n",
    "class A(str,enum.Enum):\n",
    "    DFSDS= \"SDS\"\n",
    "    sdsds=\"GEE\"\n",
    "    def __array__(cls):\n",
    "        yield from range(5)\n",
    "    # @property\n",
    "    # def __array__(cls):\n",
    "    #     return [1,2,3]\n",
    "        # ...\n",
    "MRMSFeatures.ANC_FinalForecast in np.array(MRMSFeatures.__all__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mrms data from: https://mrms.agron.iastate.edu/2022/06/01/2022060112.zip\n",
      "attempting to resolve fix zipfile\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import gzip\n",
    "import shutil\n",
    "import zipfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Literal, Iterable \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mmmpy import MRMSFeatures, MRMSRegions\n",
    "\n",
    "GZ =  \".gz\"\n",
    "READ = \"r\"\n",
    "WRITE_BINARY = \"wb\"\n",
    "\n",
    "class IAStateZip(zipfile.ZipFile):\n",
    "    def filterinfo(\n",
    "        self,\n",
    "        regions: list[MRMSRegions],\n",
    "        features: list[MRMSFeatures],\n",
    "    ) -> Iterable[zipfile.ZipInfo]:\n",
    "        df = pd.DataFrame({\"zipInfo\": self.infolist(), \"path\": self.namelist()})\n",
    "        df = df[df[\"path\"].str.endswith(GZ)]\n",
    "        df.loc[:, [\"validTime\", \"region\", \"feature\", \"name\"]] = np.vstack(\n",
    "            df[\"path\"].str.split(\"/\")\n",
    "        )\n",
    "        region_mask = np.any(\n",
    "            (np.array(regions)[:, np.newaxis] == df[\"region\"].values).T, axis=1\n",
    "        )\n",
    "        feature_mask = np.any(\n",
    "            (np.array(features)[:, np.newaxis] == df[\"feature\"].values).T, axis=1\n",
    "        )\n",
    "        yield from df.loc[(region_mask & feature_mask), \"zipInfo\"]\n",
    "\n",
    "\n",
    "def iastate_connect(start: datetime, out: Path, fix_zip: bool = True) -> Path:\n",
    "    url = start.strftime(\"https://mrms.agron.iastate.edu/%Y/%m/%d/%Y%m%d%H.zip\")\n",
    "    print(\"downloading mrms data from:\", url)\n",
    "    _, filename = url.rsplit(\"/\", maxsplit=1)\n",
    "    file = out / filename\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with file.open(WRITE_BINARY) as fd:\n",
    "            for chunk in r.iter_content(chunk_size=4096):\n",
    "                fd.write(chunk)\n",
    "    if fix_zip:\n",
    "        fix_badzip(file)\n",
    "    return file\n",
    "\n",
    "\n",
    "def fix_badzip(corrupt: Path, in_place:bool=True):\n",
    "    print(\"attempting to resolve zipfile\")\n",
    "\n",
    "    tmpfile = corrupt.parent / f\"{uuid.uuid1()}.zip\"\n",
    "\n",
    "    subprocess.call(\n",
    "        [\"zip\", \"-FF\", corrupt.as_posix(), f\"--out={tmpfile.as_posix()}\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "    )\n",
    "    if in_place:\n",
    "        corrupt.unlink()\n",
    "        shutil.move(tmpfile, corrupt)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = Path.cwd().parent / \"data\"\n",
    "    zfile = iastate_connect(\n",
    "        datetime.fromisoformat(\"2022-06-01T12\"), out=data\n",
    "    )\n",
    "    assert zfile.is_file()\n",
    "    # open the zip file\n",
    "    with IAStateZip(zfile) as zf:\n",
    "        # filter the info inside of the zip\n",
    "        for member in zf.filterinfo(regions=[\"CONUS\"], features=[\"MergedReflectivityQC\"]):\n",
    "            # split the nested product directory\n",
    "            directory, filename = member.filename.rsplit(\"/\",maxsplit=1)\n",
    "            # create a new file_path\n",
    "            file_path = zfile.parent / directory\n",
    "            if not file_path.exists():\n",
    "                file_path.mkdir(parents=True)\n",
    "            file = file_path / filename.removesuffix(GZ)\n",
    "            # read the member from the zip file\n",
    "            with zf.open(member,READ) as zref:\n",
    "                # open a new file\n",
    "                with file.open(WRITE_BINARY) as fdst:\n",
    "                    # wrote the unziped & gunziped file to the new folder\n",
    "                    fdst.write(gzip.decompress(zref.read()))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "def generate(files:tuple[Path,...]):\n",
    "    for file in files[:4]:\n",
    "        ds1 = xr.open_dataset(\n",
    "            file,\n",
    "            engine=\"cfgrib\",\n",
    "            chunks={},\n",
    "            # data_vars=\"minimal\",\n",
    "            # combine=\"nested\",\n",
    "            # concat_dim=[\"heightAboveSea\"],\n",
    "            backend_kwargs=dict(\n",
    "                mask_and_scale=True,\n",
    "                decode_times=True,\n",
    "                concat_characters=True,\n",
    "                decode_coords=True,\n",
    "                # use_cftime=\"%Y-%m\",\n",
    "                decode_timedelta=None,\n",
    "                lock=None,\n",
    "                indexpath=\"{path}.{short_hash}.idx\",\n",
    "                filter_by_keys={},\n",
    "                read_keys=[],\n",
    "                encode_cf=(\"parameter\", \"time\", \"geography\", \"vertical\"),\n",
    "                squeeze=True,\n",
    "                time_dims={\"valid_time\"},\n",
    "            ),\n",
    "        )\n",
    "        yield ds1.expand_dims(\n",
    "            {\n",
    "                \"validTime\": [ds1[\"valid_time\"].to_numpy()],\n",
    "                \"heightAboveSea\": [ds1[\"heightAboveSea\"].to_numpy()],\n",
    "            }\n",
    "        ).drop_duplicates([\"validTime\", \"heightAboveSea\"]).drop(\"valid_time\").pipe(\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "datasets = tuple(generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xr.concat(datasets,dim=\"validTime\").drop_duplicates(\"validTime\").groupby(\"validTime\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zarr\n",
    "import numpy as np\n",
    "\n",
    "teststore = data / \"test-bucket\"\n",
    "group_name = \"3DRefl\"\n",
    "\n",
    "if teststore.exists():\n",
    "    shutil.rmtree(teststore)\n",
    "for vt, ds in xr.concat(datasets,dim=\"validTime\").drop_duplicates(\"validTime\").groupby(\"validTime\"):\n",
    "    (dsname,) = ds\n",
    "    ds = ds.expand_dims({\"validTime\":[vt]}).fillna(np.nan)\n",
    "\n",
    "    ds[\"MRMS_MergedReflectivityQC\"].attrs.clear() \n",
    "    if not teststore.exists():\n",
    "        ds.to_zarr(\n",
    "            teststore,\n",
    "            mode=\"a\",\n",
    "            group=group_name,\n",
    "            compute=True,\n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        ds.drop([\"latitude\",\"longitude\",\"heightAboveSea\"]).to_zarr(\n",
    "            teststore,\n",
    "            mode=\"a\",\n",
    "            group=group_name,\n",
    "            append_dim=\"validTime\",\n",
    "            compute=True,\n",
    "        )\n",
    "\n",
    "xr.open_zarr(teststore / group_name, consolidated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "paths = pd.Series((data / \"MRMS_MergedReflectivity\").glob(\"*.grib2\"))\n",
    "vt=pd.to_datetime(paths.apply(lambda p: p.name).str.extract(r\"(\\d{8}-\\d{6})\",expand=False))\n",
    "for vt,x in paths.groupby(vt):\n",
    "    print(tuple(x))\n",
    "    # print(x)\n",
    "# pd.to_datetime(for p in paths)#pd.Series(x.name for x in (data / \"MRMS_MergedReflectivity\").glob(\"*.grib2\")).str.extract(r\"(\\d{8}-\\d{6})\",expand=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(Group)\n",
    "import zarr\n",
    "root:zarr.Group = zarr.group(teststore / group)\n",
    "def unpack_root()->tuple[zarr.Array,...]:\n",
    "    yield from (root[key] for key in (\"latitude\",\"longitude\",\"heightAboveSea\",\"validTime\"))\n",
    "x,y,z,t = unpack_root()\n",
    "tuple(z)\n",
    "# x,y,z,t :tuple[zarr.Array,...]= root[\"latitude\"], root[\"longitude\"],  root[\"heightAboveSea\"], root[\"validTime\"],\n",
    "# x,y\n",
    "# z.values\n",
    "# g:zarr.Group = zarr.open(teststore / group)\n",
    "# tuple(g.array_keys())\n",
    "# g.info.obj\n",
    "# g.info_items()\n",
    "# g.store.items()\n",
    "# g.c\n",
    "# vt: np.datetime64 = ds[\"validTime\"].values[0]\n",
    "# (vt,) = pd.to_datetime(ds[\"validTime\"].values).strftime(\"%Y-%m-%dT\")\n",
    "\n",
    "# ds[\"validTime\"].to_numpy().astype(\"datetime64[m]\")\n",
    "# vt\n",
    "# np.str\n",
    "# import zarr\n",
    "\n",
    "# g: zarr.Group = zarr.open(teststore / \"3DRefl\")\n",
    "# cs: zarr.DirectoryStore = g.chunk_store\n",
    "# g.get(\"validTime\").size\n",
    "# # print(g.get(\"validTime\"))\n",
    "# # tuple(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
